# -*- coding: utf-8 -*-
"""
Created on Fri Jul 22 12:24:04 2022

@author: Nazza
"""

import pandas as pd

import datetime

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np

from numpy.random import seed
from numpy.random import randn
from scipy.stats import shapiro
from sklearn import preprocessing
from scipy.stats import boxcox
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import sklearn.metrics as metrics
from sklearn.preprocessing import MinMaxScaler
import statsmodels.api as sm

df = pd.read_csv("bquxjob_24810ee8_1821aab8982.csv")

df['Date'] = pd.to_datetime(df['Date'], format = "%d/%m/%Y")

df_na = df[df.isna()]

df = df[df['service_contract'].notna()]

df = df[df['client'].notna()]

df = df[df['planned'].notna()]

df = df[df['actuals'].notna()]

df = df[df['revenue'] != 0]


df['quarters'] = pd.PeriodIndex(df['Date'], freq ='Q')



#df['planned'] = df['planned'].fillna(df.groupby('quarters')['planned'].transform('mean'))

#df['actuals'] = df['actuals'].fillna(df.groupby('quarters')['actuals'].transform('mean'))

correlation = df.corr()

# f= plt.figure(figsize=(12,4))

# ax=f.add_subplot(121)
# sns.distplot(df['revenue'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of revenue')

# ax=f.add_subplot(122)
# sns.distplot(np.log10(df['revenue']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of revenue in $log$ sacle')
# ax.set_xscale('log');

# g= plt.figure(figsize=(12,4))

# ax=g.add_subplot(121)
# sns.distplot(df['actuals'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of actuals')

# ax=g.add_subplot(122)
# sns.distplot(np.log10(df['actuals']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of actuals in $log$ sacle')
# ax.set_xscale('log');

# h= plt.figure(figsize=(12,4))

# ax=h.add_subplot(121)
# sns.distplot(df['planned'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of planned')

# ax=h.add_subplot(122)
# sns.distplot(np.log10(df['planned']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of planned in $log$ sacle')
# ax.set_xscale('log');

#Creates plots for the distribution of Service Contracts and copy and paste it for the clients
# f = plt.figure(figsize=(14,6))
# ax = f.add_subplot(121)
# sns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)
# ax.set_title('Violin plot of Charges vs sex')

# ax = f.add_subplot(122)
# sns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)
# ax.set_title('Violin plot of Charges vs smoker');


# # seed the random number generator
# seed(1)
# # generate univariate observations
# data = 5 * randn(100) + 50
# # normality test
# stat, p = shapiro((np.log(df['actuals'])))
# print('Statistics=%.3f, p=%.3f' % (stat, p))
# # interpret
# alpha = 0.05
# if p > alpha:
#  	print('Sample looks Gaussian (fail to reject H0)')
# else:
#  	print('Sample does not look Gaussian (reject H0)')

#From the Shapriro Wilk test we see that our data is not normally distributed even when we log the values.
#I am going to try clipping values to try to normalise

Stats = df.describe()

def find_iqr(x):
  return np.subtract(*np.percentile(x, [75, 25]))

df[['revenue', 'actuals','planned']].apply(find_iqr)


df_R = df[(df['revenue'] < 2705*1.5 + 3705.88) & (df['revenue'] > 1000 - 2705*1.5)]

df_R = df_R[(df_R['actuals'] < 32*1.5 + 46) & (df_R['actuals'] > 14 - 32*1.5)]

df_R = df_R[(df_R['planned'] < 22.371429*1.5 + 42.3714) & (df_R['planned'] > 20 - 22.371429*1.5)]

# f= plt.figure(figsize=(12,4))

# ax=f.add_subplot(121)
# sns.distplot(df_R['revenue'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of revenue')

# ax=f.add_subplot(122)
# sns.distplot(np.log10(df_R['revenue']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of revenue in $log$ sacle')
# ax.set_xscale('log');

# g= plt.figure(figsize=(12,4))

# ax=g.add_subplot(121)
# sns.distplot(df['actuals'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of actuals')

# ax=g.add_subplot(122)
# sns.distplot(np.log10(df['actuals']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of actuals in $log$ sacle')
# ax.set_xscale('log');

# h= plt.figure(figsize=(12,4))

# ax=h.add_subplot(121)
# sns.distplot(df['planned'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of planned')

# ax=h.add_subplot(122)
# sns.distplot(np.log10(df['planned']),bins=40,color='b',ax=ax)
# ax.set_title('Distribution of planned in $log$ sacle')
# ax.set_xscale('log');

# seed the random number generator
# seed(1)
# # generate univariate observations
# data = 5 * randn(100) + 50
# # normality test
# stat, p = shapiro(((np.log10(df_R['revenue']))))
# print('Statistics=%.3f, p=%.3f' % (stat, p))
# # interpret
# alpha = 0.05
# if p > alpha:
#  	print('Sample looks Gaussian (fail to reject H0)')
# else:
#  	print('Sample does not look Gaussian (reject H0)')
     

df_R.set_index('Date', inplace = True)

df_R.drop(['client','service_contract','quarters'], axis = 1, inplace = True)

# After clipping the data our data is still not normalised we are going to try to min max scale the data
d = preprocessing.normalize(df_R)
scaled_df = pd.DataFrame(d, columns=df_R.columns)
scaled_df.head()

# #seed the random number generator
# seed(1)
# # generate univariate observations
# data = 5 * randn(100) + 50
# # normality test
# stat, p = shapiro(((np.log10(df['planned']))))
# print('Statistics=%.3f, p=%.3f' % (stat, p))
# # interpret
# alpha = 0.05
# if p > alpha:
#  	print('Sample looks Gaussian (fail to reject H0)')
# else:
#  	print('Sample does not look Gaussian (reject H0)')
     
# f= plt.figure(figsize=(12,4))

# ax=f.add_subplot(121)
# sns.distplot(scaled_df['actuals'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of actuals')

# ax=f.add_subplot(121)
# sns.distplot(scaled_df['revenue'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of revenue')

# ax=f.add_subplot(121)
# sns.distplot(scaled_df['planned'],bins=50,color='r',ax=ax)
# ax.set_title('Distribution of planned')

df_fin = df.drop('quarters', axis = 1 )

df_fin = df.groupby(['Date','service_contract']).sum().reset_index()

categorical_columns = ['service_contract']

scaler = MinMaxScaler()

df_encode = pd.get_dummies(data = df_fin,columns = categorical_columns, drop_first =True)

df_encode[['planned','actuals','revenue']] = scaler.fit_transform(df_encode[['planned','actuals','revenue']])

#Consider Box-cox transformation

df_encode['Date'] = df_encode['Date'].map(datetime.datetime.toordinal)

#df_encode.drop('client', axis = 1, inplace = True)

f= plt.figure(figsize=(12,4))

#E_corr = df_encode.corr()


# Import required libraries

l = df_encode.describe()

X = df_encode.drop(['revenue','Date'], axis = 1)

y = df_encode['revenue']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)

X_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]

X_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]

theta = np.matmul(np.linalg.inv(np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train))

parameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]
columns = ['intersect:x_0=1'] + list(X.columns.values)
parameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})

lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)

# y_pred_norm =  np.matmul(X_test_0,theta)

# #Evaluvation: MSE
# J_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]

# # R_square 
# sse = np.sum((y_pred_norm - y_test)**2)
# sst = np.sum((y_test - y_test.mean())**2)
# R_square = 1 - (sse/sst)

# sk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)
# parameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))
# parameter_df

# df_fin['revenue'] = np.log10(df_fin['revenue'])

# f = plt.figure(figsize=(14,6))
# ax = f.add_subplot(121)

# sns.violinplot(x='service_contract', y='revenue',data=df_fin,palette='Wistia',ax=ax)
# ax.set_title('Violin plot of Charges vs service_contract')

# g = plt.figure(figsize=(100,8))
# ax = g.add_subplot(122)
# sns.violinplot(x='client', y='revenue',data=df,palette='magma',ax=ax)
# ax.set_title('Violin plot of Charges vs client');

y_pred_norm =  np.matmul(X_test_0,theta)

#Evaluvation: MSE
J_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]

# R_square 
sse = np.sum((y_pred_norm - y_test)**2)

sst = np.sum((y_test - y_test.mean())**2)

R_square = 1 - (sse/sst)
print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)
print('R square obtain for normal equation method is :',R_square)

y_pred_sk = lin_reg.predict(X_test)

#Evaluvation: MSE
from sklearn.metrics import mean_squared_error
J_mse_sk = mean_squared_error(y_pred_sk, y_test)

# R_square
R_square_sk = lin_reg.score(X_test,y_test)
print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)
print('R square obtain for scikit learn library is :',R_square_sk)


def regression_results(y_true, y_pred):

    # Regression metrics
    explained_variance=metrics.explained_variance_score(y_true, y_pred)
    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) 
    mse=metrics.mean_squared_error(y_true, y_pred) 
   # mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)
    r2=metrics.r2_score(y_true, y_pred)

    print('explained_variance: ', round(explained_variance,4))    
  #  print('mean_squared_log_error: ', round(mean_squared_log_error,4))
    print('r2: ', round(r2,4))
    print('MAE: ', round(mean_absolute_error,4))
    print('MSE: ', round(mse,4))
    print('RMSE: ', round(np.sqrt(mse),4))
    

    
# Linear Relationship: In linear regression the relationship between the dependent and independent variable to be linear. This can be checked by scatter ploting Actual value Vs Predicted value
# The residual error plot should be normally distributed.
# The mean of residual error should be 0 or close to 0 as much as possible
# The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot.
# Linear regession assumes that there is little or no Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor VIF* identifies correlation between independent variables and strength of that correlation.  VIF=11âˆ’R2 , If VIF >1 & VIF <5 moderate correlation, VIF < 5 critical level of multicollinearity.
# Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern.

f = plt.figure(figsize=(14,5))
ax = f.add_subplot(121)
sns.scatterplot(y_test,y_pred_norm,ax=ax,color='r')
ax.set_title('Check for Linearity:\n Actual Vs Predicted value')

ax = f.add_subplot(122)
sns.distplot((y_test - y_pred_sk),ax=ax,color='b')
ax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')
ax.set_title('Check for Residual normality & mean: \n Residual eror')


f,ax = plt.subplots(1,2,figsize=(14,6))
import scipy as sp
_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])
ax[0].set_title('Check for Multivariate Normality: \nQ-Q Plot')

sns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') 
ax[1].set_title('Check for Homoscedasticity: \nResidual Vs Predicted');

VIF = 1/(1- R_square_sk)
VIF

fig = plt.figure(figsize=(12,8))

#produce regression plots

fig = sm.graphics.plot_regress_exog(lin_reg, 'points', fig=fig)
